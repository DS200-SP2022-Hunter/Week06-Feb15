{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6_EDA_for_flights_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTpqom9vQKXF"
      },
      "source": [
        "# Lab 6:  EDA using a massive airline dataset\n",
        "\n",
        "For this lab, you'll need to access a 500MB+ dataset at kaggle.com.  First, register at kaggle.com, login and download the dataset at https://www.kaggle.com/usdot/flight-delays.  The dataset actually consists of three separate files, only one of which is huge.  \n",
        "\n",
        "Next, visit https://drive.google.com/drive/my-drive and find a place to upload this dataset.  For instance, using the \"New\" button you can create a new folder where you'll upload the three files.  To allow the code below to access the files I uploaded, I created a folder called \"data\" inside the \"Colab Notebooks\" folder."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and upload archive.zip\n",
        "\n",
        "The way I handled obtaining this dataset, after I clicked the \"download\" button at the kaggle.com site above and then logged into kaggle, was to save the `archive.zip` file on my computer.  I did not unzip this file on my computer; instead, I uploaded `archive.zip` directly into my `drive/My Drive/Colab Notebooks/data/` folder using the \"New\" button followed by \"File Upload\".  At that point, I had to figure out how to use the zip extractor within google drive, then move all three of the .csv files into the `data` folder I had created.\n",
        "\n",
        "This isn't the only way you can handle this step.  You might choose instead to unzip `archive.zip` on your own computer before uploading the three resulting .csv files to your google drive space.  It's up to you, as long as you wind up with `airlines.csv`, `airports.csv`, and `flights.csv` in a google drive folder you can access.  Frankly, I think it's probably less trouble to unzip on your own computer before uploading, but it's up to you.  "
      ],
      "metadata": {
        "id": "yOuBfbcXBz_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dbVzDcfcGs7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After your three .csv files are in place, you have to make sure that your Jupyter notebook can access them from the google.colab environment.  That's what the code above does after you run it, by mounting the drive.  You can actually get the colab to insert this code block automatically if you click the folder icon along the left side of the screen and then click the \"mount drive\" icon that appears at the top of the left margin. "
      ],
      "metadata": {
        "id": "4FB9zHBnGzIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change working directory\n",
        "\n",
        "Next, we can change the working directory (folder) in which the Jupyter notebook looks for files.  First, let's see what the current working directory is using pwd (print working directory)."
      ],
      "metadata": {
        "id": "3vnegt8zJ4Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "x-ZRolclMxef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use cd (change directory) to change the working directory to a different folder.  A space has to be entered as `\\ ` (backslash space)."
      ],
      "metadata": {
        "id": "wDfvfzcoNGCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab\\ Notebooks/data"
      ],
      "metadata": {
        "id": "4NE932imLTF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can verify that the three .csv files are in the new working directory using ls (list)."
      ],
      "metadata": {
        "id": "VxKu1ThGNclT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "zVnpxPuRMRaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional step:  Upgrade datascience library\n",
        "\n",
        "You may recall that we've occasionally had to fix some of the Python code used in the textbook.  This was particularly true in Section 8.5, where the maps were produced.  The reason the fixes were needed is that the default datascience library used by the colab Jupyter notebooks is out of date.  If you want, you can update it as follows and then the textbook code should work without modification even in Section 8.5."
      ],
      "metadata": {
        "id": "_NFzBmN8OUXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datascience"
      ],
      "metadata": {
        "id": "HsXrIly5O0OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNPXcT--QfOO"
      },
      "source": [
        "# Load the datascience library and other resources\n",
        "\n",
        "Once all the data files are in place, we can get to the Python code.  The first step, as usual, is to load the necessary python resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5kgObjIQJXU"
      },
      "source": [
        "# Load needed python resources\n",
        "from datascience import *\n",
        "import matplotlib\n",
        "matplotlib.use('Agg', warn=False)\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plots\n",
        "plots.style.use('fivethirtyeight')\n",
        "import numpy as np\n",
        "np.set_printoptions(threshold=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeyRsOFMQjVG"
      },
      "source": [
        "Next, we'll read the largest of the three files as a `Table` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATJPJK6JTMzi"
      },
      "source": [
        "# This code only works if flights.csv is in the current working directory (see above)\n",
        "flights = Table.read_table('flights.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrK6aMaNTQUR"
      },
      "source": [
        "Let's check out the columns available in the flights dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iX8Xi30TNBp"
      },
      "source": [
        "flights.labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNiM6kg8TXa2"
      },
      "source": [
        "Next, select a systematic sample of rows from the flights Table object using code similar to the Chapter 10 intro.  You'll need to figure out an appropriate value of 'gap' based on the lab instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl9XgT8rTXyQ"
      },
      "source": [
        "gap = **ENTER AN APPROPRIATE INTEGER VALUE OF gap HERE**\n",
        "start = np.random.choice(np.arange(gap))\n",
        "mySample = flights.take(np.arange(start, flights.num_rows, gap))\n",
        "print(mySample.num_rows)\n",
        "mySample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-sxSL1ITX_e"
      },
      "source": [
        "Load the airports dataset containing airport names and latitude/longitude coordinates:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LgSYfXbTYR_"
      },
      "source": [
        "airports = Table.read_table('airports.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTdcE9nATYeP"
      },
      "source": [
        "## Try to understand the next code block, especially the join method\n",
        "\n",
        "String together multiple Table-modifying methods (from the datascience module) to produce a cleaned-up version of the systematic sample called mySample (then print its number of rows and its first 10 rows):\n",
        "\n",
        "1.   Select mySample according to 'gap' and 'start'.\n",
        "2.   Then join every ORIGIN_AIRPORT in 'mysample' with the\n",
        " corresponding columns from 'airports' based on matching IATA_CODE.\n",
        "3.   Then select just the columns we need from the result.\n",
        "4.   Then relabel some of the columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T10NXfo1TYue"
      },
      "source": [
        "mySample = (flights.take(np.arange(start, flights.num_rows, gap))\n",
        "                   .join('ORIGIN_AIRPORT', airports, 'IATA_CODE')\n",
        "                   .select('MONTH', 'DAY', 'ORIGIN_AIRPORT', \n",
        "                           'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', \n",
        "                           'DEPARTURE_DELAY', 'AIRPORT',\n",
        "                           'LATITUDE', 'LONGITUDE')\n",
        "                   .relabeled('ORIGIN_AIRPORT', 'ORIGIN')\n",
        "                   .relabeled('DESTINATION_AIRPORT', 'DESTINATION')\n",
        "                   .relabeled('DEPARTURE_DELAY', 'DELAY')\n",
        "                   .relabeled('AIRPORT', 'ORIGIN_NAME')\n",
        "            )\n",
        "print(mySample.num_rows)\n",
        "mySample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoALcujETY72"
      },
      "source": [
        "Create a new column with the approximate day of year. There are better, more accurate ways to do this, but this method that approximates each month by 30 days will work for this purpose:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5tM0e-3UeFq"
      },
      "source": [
        "mySample = mySample.with_column(\n",
        "             'APPROX_DAY_OF_YEAR', \n",
        "             30*(mySample.column('MONTH')-1) + mySample.column('DAY'))\n",
        "mySample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrzgL6WJUeua"
      },
      "source": [
        "Recall that a scatterplot depicts the relationship between two quantitative measurement columns.  Create a scatterplot using the 'LATITUDE' and 'DELAY' columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR1ck_htUfx4"
      },
      "source": [
        "mySample.scatter('LATITUDE', 'DELAY')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAblkOH4Uf-w"
      },
      "source": [
        "# EXTRA CODE\n",
        "The code below is not strictly necessary for the lab assignment.  It is included to illustrate some potentially interesting directions you could take your own investigation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VvJn0PFUgUo"
      },
      "source": [
        "# Create a histogram of LATITUDE\n",
        "mySample.hist('LATITUDE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column that splits the airports into high vs. low latitude\n",
        "# based on a cutoff you define:\n",
        "LatCut = **ENTER A VALUE THAT MAKES SENSE HERE**\n",
        "mySample = mySample.with_column(\n",
        "              'HIGH_LAT', mySample.column('LATITUDE') > LatCut)\n",
        "mySample"
      ],
      "metadata": {
        "id": "kNs44dU8S9kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Figure out how many airports are \"high latitude\" vs. \"low latitude\"\n",
        "mySample.group('HIGH_LAT')"
      ],
      "metadata": {
        "id": "0U1Sj3jwS3Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the means for the high- and low- latitude airports.\n",
        "# We're using the 'nanmean' method in numpy to ignore the \n",
        "# nan (not a number) values\n",
        "mySample.group('HIGH_LAT', np.nanmean)"
      ],
      "metadata": {
        "id": "wbE_4pZGS0c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the previous idea to find the mean difference automatically\n",
        "Observed_mean_difference = np.diff(mySample.group('HIGH_LAT', np.nanmean)\n",
        "                                           .column('DELAY nanmean'))[0]\n",
        "Observed_mean_difference"
      ],
      "metadata": {
        "id": "QDWMgdC-Sx-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that will reshuffle the DELAY values and then return\n",
        "# the mean difference statistic for the shuffled table.\n",
        "# This simulates from the null hypothesis distribution of the \n",
        "# mean difference statistic.\n",
        "def simulated_mean_difference_under_null():\n",
        "    a=(mySample.sample(with_replacement=False)\n",
        "               .select('DELAY')\n",
        "               .with_column('HIGH_LAT', mySample.column('HIGH_LAT')))\n",
        "    return (np.diff(a.group('HIGH_LAT', np.nanmean)\n",
        "                     .column('DELAY nanmean'))[0])"
      ],
      "metadata": {
        "id": "A3JJGGMISvbj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate 5000 draws from the null hypothesis distribution of the \n",
        "# mean difference (and return the result as a numpy array--not the same \n",
        "# as a datascience Table)\n",
        "H0_means = make_array()\n",
        "for i in np.arange(5000):\n",
        "    H0_means = np.append(H0_means, simulated_mean_difference_under_null())"
      ],
      "metadata": {
        "id": "yWQphnc6Spov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a table with the 5000 H0 (null hypothesis) values and then \n",
        "# create a histogram.\n",
        "# Also add the observed value of the sample statistic as a red dot \n",
        "# along the x-axis.\n",
        "Table().with_column(\n",
        "    'Count in a Random Sample', H0_means\n",
        ").hist(bins = np.arange(-12.5, 12.5, 1))\n",
        "plots.scatter(Observed_mean_difference, 0, color='red', s=30);"
      ],
      "metadata": {
        "id": "dbbqzOmDSmH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}